{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is mostly from\n",
    "\n",
    "https://mlexplained.com/2019/01/30/an-in-depth-tutorial-to-allennlp-from-basics-to-elmo-and-bert/#more-853\n",
    "\n",
    "with slight modification and annotation by YL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import *\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from overrides import overrides\n",
    "\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.token_indexers import TokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.nn import util as nn_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "    \n",
    "    def set(self, key, val):\n",
    "        self[key] = val\n",
    "        setattr(self, key, val)\n",
    "        \n",
    "config = Config(\n",
    "    testing=False,\n",
    "    seed=1,\n",
    "    batch_size=64,\n",
    "    lr=5e-5,\n",
    "    epochs=5,\n",
    "    hidden_sz=64,\n",
    "    max_seq_len=100, # necessary to limit memory usage\n",
    "    max_vocab_size=100000,\n",
    ")\n",
    "\n",
    "bert_flavour = \"bert-base-multilingual-cased\"\n",
    "# bert_flavour = \"bert-base-chinese\"\n",
    "\n",
    "#if true, read whole dataset and load pretrained weight then run fine tune training\n",
    "#if false, read small dataset and load finetunned weight and skip fine tune training\n",
    "retrain = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.common.checks import ConfigurationError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USE_GPU = torch.cuda.is_available()\n",
    "USE_GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seed manually to replicate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd79233cf90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.data.dataset_readers import DatasetReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: '吹水台',\n",
       " 4: '手機台',\n",
       " 5: '時事台',\n",
       " 6: '體育台',\n",
       " 7: '娛樂台',\n",
       " 8: '動漫台',\n",
       " 9: 'Apps台',\n",
       " 10: '遊戲台',\n",
       " 11: '影視台',\n",
       " 12: '講故台',\n",
       " 13: '潮流台',\n",
       " 14: '上班台',\n",
       " 15: '財經台',\n",
       " 16: '飲食台',\n",
       " 17: '旅遊台',\n",
       " 18: '學術台',\n",
       " 19: '校園台',\n",
       " 20: '汽車台',\n",
       " 21: '音樂台',\n",
       " 22: '硬件台',\n",
       " 23: '攝影台',\n",
       " 24: '玩具台',\n",
       " 25: '寵物台',\n",
       " 26: '軟件台',\n",
       " 27: '活動台',\n",
       " 28: '站務台',\n",
       " 29: '成人台',\n",
       " 30: '感情台',\n",
       " 31: '創意台',\n",
       " 32: '黑\\u3000洞',\n",
       " 33: '政事台',\n",
       " 34: '直播台',\n",
       " 35: '電訊台',\n",
       " 36: '健康台'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_ids = pickle.load(open(DATA_ROOT / \"data/cat_id_lut.pkl\",\"rb\"))\n",
    "cat_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ncats = max(cat_ids.keys()) - min(cat_ids.keys())+1\n",
    "ncats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.fields import TextField, MetadataField, ArrayField\n",
    "\n",
    "class LihkgDatasetReader(DatasetReader):\n",
    "    def __init__(self, tokenizer: Callable[[str], List[str]]=lambda x: x.split(),\n",
    "                 token_indexers: Dict[str, TokenIndexer] = None,\n",
    "                 max_seq_len: Optional[int]=config.max_seq_len) -> None:\n",
    "        super().__init__(lazy=False)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    @overrides\n",
    "    def text_to_instance(self, tokens: List[Token],\n",
    "                         label: int=None) -> Instance:\n",
    "        sentence_field = TextField(tokens, self.token_indexers)\n",
    "        fields = {\"tokens\": sentence_field}\n",
    "        \n",
    "        \n",
    "        labels = np.zeros(ncats)\n",
    "        if label is not None:\n",
    "            labels[label-1]=1 #one hot encode\n",
    "            \n",
    "        label_field = ArrayField(array=labels)\n",
    "        fields[\"label\"] = label_field\n",
    "\n",
    "        return Instance(fields)\n",
    "    \n",
    "    @overrides\n",
    "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
    "        df = pd.read_csv(file_path)\n",
    "        if config.testing: df = df.head(1000)\n",
    "        for i, row in df.iterrows():\n",
    "            yield self.text_to_instance(\n",
    "                [Token(x) for x in self.tokenizer(row[\"title\"])],\n",
    "                row[\"cat_id\"],\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare token handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.token_indexers import PretrainedBertIndexer\n",
    "\n",
    "token_indexer = PretrainedBertIndexer(\n",
    "    pretrained_model=str(DATA_ROOT / (\"pretrain/%s-vocab.txt\"%bert_flavour)),\n",
    "    max_pieces=config.max_seq_len,\n",
    "    do_lowercase=True,\n",
    " )\n",
    "# apparently we need to truncate the sequence here, which is a stupid design decision\n",
    "def tokenizer(s: str):\n",
    "    return token_indexer.wordpiece_tokenizer(s)[:config.max_seq_len - 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = LihkgDatasetReader(\n",
    "    tokenizer=tokenizer,\n",
    "    token_indexers={\"tokensIdxers1\": token_indexer}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14609it [00:05, 2767.26it/s]\n",
      "3661it [00:01, 2846.54it/s]\n",
      "2035it [00:00, 3016.07it/s]\n"
     ]
    }
   ],
   "source": [
    "if retrain:\n",
    "    train_ds, val_ds, test_ds = (reader.read(DATA_ROOT / fname) for fname in [\"data/lihkg_posts_20190227_train.csv\", \"data/lihkg_posts_20190227_val.csv\", \"data/lihkg_posts_20190227_test.csv\"])\n",
    "else:\n",
    "    train_ds, val_ds, test_ds = (reader.read(DATA_ROOT / fname) for fname in [\"data/lihkg_posts_train.csv\", \"data/lihkg_posts_val.csv\", \"data/lihkg_posts_test.csv\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_ds is a list of \"instance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14609"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<allennlp.data.instance.Instance at 0x7fd710b27b70>,\n",
       " <allennlp.data.instance.Instance at 0x7fd710b27fd0>,\n",
       " <allennlp.data.instance.Instance at 0x7fd71109f1d0>,\n",
       " <allennlp.data.instance.Instance at 0x7fd71109f2b0>,\n",
       " <allennlp.data.instance.Instance at 0x7fd71109f5c0>,\n",
       " <allennlp.data.instance.Instance at 0x7fd71109fc18>,\n",
       " <allennlp.data.instance.Instance at 0x7fd7110a33c8>,\n",
       " <allennlp.data.instance.Instance at 0x7fd7110a3780>,\n",
       " <allennlp.data.instance.Instance at 0x7fd7110a3ba8>,\n",
       " <allennlp.data.instance.Instance at 0x7fd7110a8278>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what is inside an instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': <allennlp.data.fields.text_field.TextField at 0x7fd7a5212208>,\n",
       " 'label': <allennlp.data.fields.array_field.ArrayField at 0x7fd7a3880ef0>}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0].fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': [好, ##想, ##玩, ##三, ##國, ##志, ##11],\n",
       " '_token_indexers': {'tokensIdxers1': <allennlp.data.token_indexers.wordpiece_indexer.PretrainedBertIndexer at 0x7fd7a0640278>},\n",
       " '_indexed_tokens': None,\n",
       " '_indexer_name_to_indexed_token': None}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(train_ds[0].fields[\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'array': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.]), 'padding_value': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(train_ds[0].fields['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need to build the vocab: all that is handled by the token indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iterator is responsible for batching the data and preparing it for input into the model. We'll use the BucketIterator that batches text sequences of smilar lengths together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.iterators import BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = BucketIterator(batch_size=config.batch_size, \n",
    "                          #sorting_keys=[(\"tokens\", \"num_tokens\")],\n",
    "                          sorting_keys=[(\"tokens\", \"tokensIdxers1_length\")], #same as num_token since only 1 tokenIndexer\n",
    "                          max_instances_in_memory = 1000,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to tell the iterator how to numericalize the text data. We do this by passing the vocabulary to the iterator. This step is easy to forget so be careful! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator.index_with(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(iterator(train_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['tokens', 'label'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['tokensIdxers1', 'tokensIdxers1-offsets', 'tokensIdxers1-type-ids', 'mask'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"tokens\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   101,  22257,   4163,  ...,      0,      0,      0],\n",
       "        [   101,   8595, 117791,  ...,      0,      0,      0],\n",
       "        [   101,   2796, 112987,  ...,      0,      0,      0],\n",
       "        ...,\n",
       "        [   101,   2104, 112440,  ...,      0,      0,      0],\n",
       "        [   101,    164, 117483,  ...,      0,      0,      0],\n",
       "        [   101,    113, 117673,  ...,    102,      0,      0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"tokens\"][\"tokensIdxers1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"tokens\"][\"tokensIdxers1\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 36])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['label'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
    "from allennlp.nn.util import get_text_field_mask\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder\n",
    "\n",
    "class BaselineModel(Model):\n",
    "    def __init__(self, word_embeddings: TextFieldEmbedder,\n",
    "                 encoder: Seq2VecEncoder,\n",
    "                 out_sz: int=ncats):\n",
    "        super().__init__(vocab)\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.encoder = encoder\n",
    "        self.projection = nn.Linear(self.encoder.get_output_dim(), out_sz)\n",
    "        self.loss = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    def forward(self, tokens: Dict[str, torch.Tensor],\n",
    "                label: torch.Tensor) -> torch.Tensor:\n",
    "        mask = get_text_field_mask(tokens)\n",
    "        embeddings = self.word_embeddings(tokens)\n",
    "        state = self.encoder(embeddings, mask)\n",
    "        class_logits = self.projection(state)\n",
    "        \n",
    "        output = {\"class_logits\": class_logits}\n",
    "        output[\"loss\"] = self.loss(class_logits, label)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders.bert_token_embedder import PretrainedBertEmbedder\n",
    "\n",
    "bert_embedder = PretrainedBertEmbedder(\n",
    "        pretrained_model=str(DATA_ROOT / (\"pretrain/%s.tar.gz\"% bert_flavour)),\n",
    "        requires_grad=True, #Finetune BERT weight or not\n",
    "        top_layer_only=True, # if False, embedding is weighted average of all layers in BERT\n",
    ")\n",
    "word_embeddings: TextFieldEmbedder = BasicTextFieldEmbedder({\"tokensIdxers1\": bert_embedder},\n",
    "                                                            # we'll be ignoring masks so we'll need to set this to True\n",
    "                                                           allow_unmatched_keys = True)\n",
    "\n",
    "# this is because the bert_indexer generate more then 1 output\n",
    "# ordinary indexer just gen 1 output with key the same as the indexer i.e. \"tokensIdxess1\"\n",
    "# bert indexer gen output with key 'tokensIdxers1', 'tokensIdxers1-offsets', 'tokensIdxers1-type-ids', 'mask'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_DIM = word_embeddings.get_output_dim()\n",
    "\n",
    "class BertSentencePooler(Seq2VecEncoder):\n",
    "    def forward(self, embs: torch.tensor, \n",
    "                mask: torch.tensor=None) -> torch.tensor:\n",
    "        # extract first token tensor\n",
    "        return embs[:, 0]\n",
    "    \n",
    "    @overrides\n",
    "    def get_output_dim(self) -> int:\n",
    "        return BERT_DIM\n",
    "    \n",
    "encoder = BertSentencePooler(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how simple and modular the code for initializing the model is. All the complexity is delegated to each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaselineModel(\n",
    "    word_embeddings, \n",
    "    encoder, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_GPU: model.cuda()\n",
    "else: model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = nn_util.move_to_device(batch, 0 if USE_GPU else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = batch[\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokensIdxers1': tensor([[   101,  22257,   4163,  ...,      0,      0,      0],\n",
       "         [   101,   8595, 117791,  ...,      0,      0,      0],\n",
       "         [   101,   2796, 112987,  ...,      0,      0,      0],\n",
       "         ...,\n",
       "         [   101,   2104, 112440,  ...,      0,      0,      0],\n",
       "         [   101,    164, 117483,  ...,      0,      0,      0],\n",
       "         [   101,    113, 117673,  ...,    102,      0,      0]]),\n",
       " 'tokensIdxers1-offsets': tensor([[ 1,  2,  3,  ...,  0,  0,  0],\n",
       "         [ 1,  2,  3,  ...,  0,  0,  0],\n",
       "         [ 1,  2,  3,  ...,  0,  0,  0],\n",
       "         ...,\n",
       "         [ 1,  2,  3,  ...,  0,  0,  0],\n",
       "         [ 1,  2,  3,  ...,  0,  0,  0],\n",
       "         [ 1,  2,  3,  ..., 27, 28,  0]]),\n",
       " 'tokensIdxers1-type-ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 0]])}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 0]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = get_text_field_mask(tokens)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0933, -0.0759, -0.0174,  ..., -0.5220,  0.4285,  0.3026],\n",
       "        [-0.0986,  0.0405,  0.0439,  ..., -0.4200,  0.3379,  0.0592],\n",
       "        [-0.1062,  0.0241,  0.1827,  ..., -0.6862,  0.1846,  0.2976],\n",
       "        ...,\n",
       "        [-0.1739, -0.0776,  0.4444,  ..., -0.4775,  0.3417,  0.1587],\n",
       "        [-0.0259, -0.0951,  0.0562,  ..., -0.6593,  0.2696,  0.0848],\n",
       "        [-0.1038, -0.2220,  0.0832,  ..., -0.3604,  0.2358,  0.1642]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = model.word_embeddings(tokens)\n",
    "state = model.encoder(embeddings, mask)\n",
    "class_logits = model.projection(state)\n",
    "class_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_logits': tensor([[ 0.1668,  0.0560,  0.0265,  ..., -0.2267,  0.1827,  0.1101],\n",
       "         [-0.3499, -0.0192,  0.2075,  ..., -0.4669,  0.3533,  0.2253],\n",
       "         [-0.1326, -0.0518,  0.3157,  ..., -0.5325,  0.2581,  0.0155],\n",
       "         ...,\n",
       "         [-0.2375, -0.0828,  0.1574,  ..., -0.5315,  0.0620, -0.0714],\n",
       "         [-0.2675,  0.1637,  0.2599,  ..., -0.5501,  0.1942,  0.2000],\n",
       "         [-0.0214, -0.0584,  0.2488,  ..., -0.6487,  0.3309,  0.1936]],\n",
       "        grad_fn=<AddmmBackward>),\n",
       " 'loss': tensor(0.7198, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(**batch)[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7201, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our \"encoder\" just takes the 1st token's embedding, thus has no free parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.grad for x in list(model.encoder.parameters())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=config.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.training.trainer import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    iterator=iterator,\n",
    "    train_dataset=train_ds,\n",
    "    validation_dataset=val_ds,\n",
    "    cuda_device=0 if USE_GPU else -1,\n",
    "    num_epochs=config.epochs,\n",
    "    patience=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_filename = \"chkpoints/%s-finetune.pth\" % bert_flavour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not retrain:\n",
    "    with open(DATA_ROOT / weight_filename, 'rb') as f:\n",
    "        if USE_GPU:\n",
    "            model.load_state_dict(torch.load(f))\n",
    "        else:\n",
    "            model.load_state_dict(torch.load(f, map_location='cpu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "if retrain:\n",
    "    metrics = trainer.train()\n",
    "    with open(DATA_ROOT / weight_filename, 'wb') as f:\n",
    "        torch.save(model.state_dict(), f)\n",
    "    print (metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Predictions in bulk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.iterators import DataIterator\n",
    "from tqdm import tqdm\n",
    "from scipy.special import expit # the sigmoid function\n",
    "\n",
    "def tonp(tsr): return tsr.detach().cpu().numpy()\n",
    "\n",
    "class Predictor:\n",
    "    def __init__(self, model: Model, iterator: DataIterator,\n",
    "                 cuda_device: int=-1) -> None:\n",
    "        self.model = model\n",
    "        self.iterator = iterator\n",
    "        self.cuda_device = cuda_device\n",
    "        \n",
    "    def _extract_data(self, batch) -> np.ndarray:\n",
    "        out_dict = self.model(**batch)\n",
    "        return expit(tonp(out_dict[\"class_logits\"]))\n",
    "    \n",
    "    def predict(self, ds: Iterable[Instance]) -> np.ndarray:\n",
    "        pred_generator = self.iterator(ds, num_epochs=1, shuffle=False)\n",
    "        self.model.eval()\n",
    "        pred_generator_tqdm = tqdm(pred_generator,\n",
    "                                   total=self.iterator.get_num_batches(ds))\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for batch in pred_generator_tqdm:\n",
    "                batch = nn_util.move_to_device(batch, self.cuda_device)\n",
    "                preds.append(self._extract_data(batch))\n",
    "        return np.concatenate(preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.iterators import BasicIterator\n",
    "# iterate over the dataset without changing its order\n",
    "seq_iterator = BasicIterator(batch_size=64)\n",
    "seq_iterator.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|▎         | 1/32 [00:04<02:21,  4.55s/it]\u001b[A\n",
      "  6%|▋         | 2/32 [00:07<02:02,  4.08s/it]\u001b[A\n",
      "  9%|▉         | 3/32 [00:10<01:47,  3.71s/it]\u001b[A\n",
      " 12%|█▎        | 4/32 [00:14<01:44,  3.72s/it]\u001b[A\n",
      " 16%|█▌        | 5/32 [00:18<01:42,  3.81s/it]\u001b[A\n",
      " 19%|█▉        | 6/32 [00:21<01:37,  3.74s/it]\u001b[A\n",
      " 22%|██▏       | 7/32 [00:24<01:28,  3.52s/it]\u001b[A\n",
      " 25%|██▌       | 8/32 [00:28<01:27,  3.63s/it]\u001b[A\n",
      " 28%|██▊       | 9/32 [00:31<01:15,  3.30s/it]\u001b[A\n",
      " 31%|███▏      | 10/32 [00:33<01:07,  3.09s/it]\u001b[A\n",
      " 34%|███▍      | 11/32 [00:36<01:03,  3.04s/it]\u001b[A\n",
      " 38%|███▊      | 12/32 [00:39<01:01,  3.08s/it]\u001b[A\n",
      " 41%|████      | 13/32 [00:42<00:59,  3.11s/it]\u001b[A\n",
      " 44%|████▍     | 14/32 [00:46<00:57,  3.22s/it]\u001b[A\n",
      " 47%|████▋     | 15/32 [00:49<00:53,  3.16s/it]\u001b[A\n",
      " 50%|█████     | 16/32 [00:52<00:51,  3.22s/it]\u001b[A\n",
      " 53%|█████▎    | 17/32 [00:56<00:50,  3.39s/it]\u001b[A\n",
      " 56%|█████▋    | 18/32 [01:00<00:48,  3.47s/it]\u001b[A\n",
      " 59%|█████▉    | 19/32 [01:03<00:44,  3.46s/it]\u001b[A\n",
      " 62%|██████▎   | 20/32 [01:07<00:42,  3.58s/it]\u001b[A\n",
      " 66%|██████▌   | 21/32 [01:11<00:38,  3.53s/it]\u001b[A\n",
      " 69%|██████▉   | 22/32 [01:14<00:35,  3.52s/it]\u001b[A\n",
      " 72%|███████▏  | 23/32 [01:16<00:28,  3.14s/it]\u001b[A\n",
      " 75%|███████▌  | 24/32 [01:19<00:25,  3.13s/it]\u001b[A\n",
      " 78%|███████▊  | 25/32 [01:23<00:23,  3.29s/it]\u001b[A\n",
      " 81%|████████▏ | 26/32 [01:27<00:20,  3.41s/it]\u001b[A\n",
      " 84%|████████▍ | 27/32 [01:30<00:16,  3.35s/it]\u001b[A\n",
      " 88%|████████▊ | 28/32 [01:33<00:12,  3.22s/it]\u001b[A\n",
      " 91%|█████████ | 29/32 [01:36<00:09,  3.08s/it]\u001b[A\n",
      " 94%|█████████▍| 30/32 [01:38<00:05,  2.90s/it]\u001b[A\n",
      " 97%|█████████▋| 31/32 [01:41<00:03,  3.04s/it]\u001b[A\n",
      "100%|██████████| 32/32 [01:45<00:00,  3.16s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "predictor = Predictor(model, seq_iterator, cuda_device=0 if USE_GPU else -1)\n",
    "# train_preds = predictor.predict(train_ds) \n",
    "test_preds = predictor.predict(test_ds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.0256435e-03, 5.2302953e-09, 6.6384516e-09, 2.7938961e-04,\n",
       "       5.1119551e-04, 9.0638740e-04, 1.3369879e-04, 2.3170123e-04,\n",
       "       2.8342550e-04, 9.9209142e-01, 1.9863427e-04, 3.5612855e-05,\n",
       "       4.8792692e-05, 1.2237292e-04, 2.4284588e-04, 1.0908637e-04,\n",
       "       8.8957328e-05, 2.2884137e-04, 2.1211820e-04, 1.9855321e-04,\n",
       "       2.1967693e-04, 1.9203312e-03, 9.6240437e-05, 3.8534845e-04,\n",
       "       2.2629087e-05, 2.3867024e-04, 1.2670888e-04, 2.8933393e-05,\n",
       "       1.3840353e-04, 1.5202230e-04, 2.9313882e-04, 8.0075715e-06,\n",
       "       7.5074902e-05, 7.0124120e-04, 4.0976520e-05, 2.5300418e-05],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Predictions per instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.predictors.sentence_tagger import SentenceTaggerPredictor\n",
    "\n",
    "model.eval()\n",
    "tagger = SentenceTaggerPredictor(model, reader)\n",
    "\n",
    "def predict_cat(s):\n",
    "    logits = tagger.predict(s)['class_logits']\n",
    "    probs = expit(logits)\n",
    "    cat_ranked = probs.argsort()[::-1]\n",
    "    for cat in cat_ranked[:5]:\n",
    "        print (\"%2d %s %0.2f\" % (cat+1, cat_ids.get(cat+1,'Nil'), probs[cat]))\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 創意台 0.37\n",
      "14 上班台 0.19\n",
      "30 感情台 0.16\n",
      " 1 吹水台 0.13\n",
      "15 財經台 0.03\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.30229738e-01, 9.68756547e-08, 7.82302534e-08, 4.29208223e-03,\n",
       "       4.03621973e-04, 1.62166415e-03, 3.20505841e-03, 1.61382171e-03,\n",
       "       2.56355792e-03, 1.33242549e-03, 5.55419318e-04, 2.57377612e-03,\n",
       "       2.09480984e-02, 1.89542223e-01, 3.44811291e-02, 2.37401844e-03,\n",
       "       6.74083538e-04, 1.44032373e-02, 9.29753838e-04, 8.42620630e-04,\n",
       "       1.44957371e-03, 1.38655105e-03, 3.23537888e-03, 1.04428094e-02,\n",
       "       8.03332830e-03, 6.41325793e-03, 8.64076251e-03, 1.17262867e-03,\n",
       "       9.41072244e-03, 1.59753568e-01, 3.65698823e-01, 1.67630522e-04,\n",
       "       3.44324442e-05, 3.72896553e-05, 5.06894585e-05, 1.68918455e-03])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_cat('有無巴絲想創業賣暖手杯？')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 學術台 0.62\n",
      " 5 時事台 0.20\n",
      "31 創意台 0.07\n",
      "33 政事台 0.06\n",
      " 1 吹水台 0.03\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2.82041618e-02, 5.82127678e-09, 7.75350130e-09, 2.84836857e-04,\n",
       "       1.99179655e-01, 1.25203246e-02, 1.38632244e-02, 1.19707607e-03,\n",
       "       6.08758834e-05, 1.91235374e-04, 8.16013017e-03, 2.23782212e-03,\n",
       "       6.44997313e-04, 1.68288568e-03, 1.22917348e-02, 3.67496999e-03,\n",
       "       5.65681267e-04, 6.18406327e-01, 6.12195146e-03, 1.66929984e-05,\n",
       "       3.92323312e-04, 1.94203670e-05, 7.96323730e-05, 7.55593419e-05,\n",
       "       7.97885011e-05, 9.86057605e-05, 3.30366512e-04, 2.76948116e-04,\n",
       "       2.25995666e-04, 1.55878164e-03, 6.50506960e-02, 5.58609565e-06,\n",
       "       5.92488836e-02, 1.21655289e-04, 1.49689708e-05, 1.06933874e-03])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_cat('中國四大偉人：毛澤東、鄧小平、習近平、貧僧')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7 娛樂台 0.57\n",
      "21 音樂台 0.52\n",
      " 1 吹水台 0.01\n",
      "30 感情台 0.00\n",
      "11 影視台 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([5.28733937e-03, 5.97462056e-09, 5.50169581e-09, 4.81850084e-05,\n",
       "       1.27002943e-04, 5.00252889e-04, 5.71169596e-01, 2.75188324e-04,\n",
       "       6.48161862e-05, 6.40406414e-05, 1.48426226e-03, 5.59883542e-05,\n",
       "       1.14716496e-03, 1.26386932e-04, 1.83471336e-04, 1.08561192e-04,\n",
       "       1.69060101e-04, 2.19638183e-04, 1.97755341e-04, 4.15588344e-05,\n",
       "       5.18831785e-01, 2.37996032e-04, 5.14486406e-05, 2.55534965e-05,\n",
       "       1.82427066e-05, 3.59101682e-05, 2.60833461e-04, 3.24450924e-05,\n",
       "       6.08080002e-05, 1.67675952e-03, 7.90514695e-04, 2.78521552e-06,\n",
       "       3.17497633e-05, 3.09394175e-05, 5.84617594e-06, 6.36811578e-05])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_cat(\"[ALL IN US] ITZY討論區(1) IT'z Different大發~~~~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 寵物台 0.46\n",
      "16 飲食台 0.46\n",
      "31 創意台 0.06\n",
      " 1 吹水台 0.04\n",
      "18 學術台 0.02\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4.45906485e-02, 1.05464807e-08, 9.91994384e-09, 1.41109741e-03,\n",
       "       1.11162316e-03, 2.50632383e-03, 7.80373077e-04, 7.57171726e-04,\n",
       "       5.90223057e-04, 9.63383967e-04, 3.18183019e-04, 2.91407163e-03,\n",
       "       3.38568138e-04, 1.61592621e-02, 5.02451719e-03, 4.56044820e-01,\n",
       "       5.27513357e-04, 1.93788835e-02, 1.53796477e-03, 7.29677135e-04,\n",
       "       9.06565308e-04, 1.73104754e-03, 5.61649490e-04, 3.32808541e-04,\n",
       "       4.64066695e-01, 8.43469731e-04, 1.29416916e-04, 4.77583143e-04,\n",
       "       9.43065140e-04, 4.89875325e-03, 6.28404662e-02, 9.74792941e-05,\n",
       "       3.28291810e-04, 1.20680593e-04, 7.02659955e-05, 1.95548360e-03])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_cat(\"有無巴打食過狗糧？ 咩味？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7 娛樂台 0.86\n",
      "21 音樂台 0.09\n",
      " 8 動漫台 0.08\n",
      " 1 吹水台 0.00\n",
      "11 影視台 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4.16704358e-03, 1.18593103e-08, 6.94446923e-09, 1.25296766e-04,\n",
       "       2.43168394e-04, 2.12584806e-03, 8.62017581e-01, 8.47825680e-02,\n",
       "       5.36677543e-04, 1.20059663e-03, 2.29705821e-03, 3.15451946e-04,\n",
       "       4.26230405e-04, 1.79752870e-04, 1.38878525e-04, 1.66694575e-04,\n",
       "       1.60906888e-04, 1.39018703e-04, 2.81078969e-04, 7.40532153e-05,\n",
       "       9.47564088e-02, 4.38377990e-05, 1.01587204e-04, 1.46183797e-04,\n",
       "       3.73129194e-05, 4.33523894e-05, 2.61034476e-04, 1.10513337e-04,\n",
       "       2.07680987e-03, 1.64214797e-03, 2.04445787e-03, 5.43810165e-06,\n",
       "       5.33113101e-05, 1.73729902e-04, 5.03013863e-06, 4.98549227e-05])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_cat(\"西野カナ fans揮手區(5) 活動休止中\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 上班台 0.70\n",
      "31 創意台 0.15\n",
      " 1 吹水台 0.12\n",
      "30 感情台 0.04\n",
      " 5 時事台 0.02\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.20766512e-01, 7.80079108e-09, 7.09452324e-09, 1.55773708e-03,\n",
       "       2.28576223e-02, 7.01079612e-03, 3.17047582e-03, 7.62227088e-04,\n",
       "       9.17044176e-05, 1.35244559e-03, 1.27855468e-03, 1.45804302e-03,\n",
       "       1.51424715e-02, 6.98473609e-01, 6.85537399e-03, 8.99559108e-03,\n",
       "       1.13343704e-04, 1.27359278e-02, 4.46021036e-03, 2.48043903e-04,\n",
       "       2.02212525e-04, 5.48392768e-03, 4.14365124e-05, 2.67368337e-04,\n",
       "       1.04350359e-04, 4.57699812e-03, 7.52210175e-05, 5.02096609e-05,\n",
       "       8.49452806e-04, 4.42502162e-02, 1.45827749e-01, 8.49899383e-06,\n",
       "       2.82615207e-04, 8.42950162e-05, 1.39365514e-04, 3.89645649e-03])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_cat(\"Annual Dinner 抽中左Dyson 風筒，HR打黎話要收返\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
